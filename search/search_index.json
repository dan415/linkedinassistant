{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Linkedin GPT-based Posting Creator Assistant I created this project because I wanted to create posts about cool articles and papers that would come up every week, but did not have the time for it. So I decided review them and post them. to create a program that would generate the posts for me, and I would only need to review them and post them. The program is based on the OpenAI's LLMs, through Langchain, but could be easily adapted to any other LLM model. All design decisions are based on the idea that this program is meant to be run as a Windows service locally. That's why all agents constantly update their status on a local file (so that if the laptop turns off abruptly, the status is not lost). As of now, I still need to implement the behaviour for the stop signal though. This program is comprised of 7 main components: - Information Searching: This component is in charge of searching for information on the internet. It is based on the Google Search API, and it is implemented in the searcher.py file. It is a simple class that receives a query and returns a list of results. It is meant to be used by the search_agent.py agent. - Telegram Bot: This component is in charge of communicating with the Telegram Bot API. It is implemented in the telegram_bot.py file. It is a simple class that receives a message and sends it to the Telegram Bot API. It is meant to be used by the telegram_agent.py agent. - Linkedn Component: This component is in charge of generating the posts. It is implemented in the posting.py file. It is a simple class that receives a query and returns a list of results. It is meant to be used by the posting_agent.py agent. - LLM Component: This component is in charge of generating the posts. It is implemented in the llm.py file. It is a simple class that receives a query and returns a list of results. It is meant to be used by the llm_agent.py agent. - Windows Service: This component is in charge of running the program as a Windows service. It is implemented in the service.py file. It is a simple class that receives a query and returns a list of results. It is meant to be used by the service_agent.py agent. - Main: This component is in charge of running the program. It is implemented in the main.py file. It is a simple class that receives a query and returns a list of results. It is meant to be used by the main_agent.py agent. Installation In order to install the program, you need to do the following: Install the dependencies in conda: conda env create -f environment.yml Create a Telegram Bot and get the API key. Step 1. Open the Telegram app on your computer/phone To create a Telegram bot, you'll need to have the Telegram app installed on your computer. If you don't have it already, you can download it from the Telegram website. Step 2. Connect to BotFather BotFather is a bot created by Telegram that allows you to create and manage your own bots. To connect to BotFather, search for \"@BotFather\" in the Telegram app and click on the result to start a conversation. Step 3. Select the New Bot option In the conversation with BotFather, select the \"New Bot\" option to start creating your new bot. BotFather will guide you through the rest of the process. Step 4. Add a bot name Next, BotFather will ask you to provide a name for your bot. Choose a name that accurately reflects the purpose of your bot and is easy to remember. Step 5. Choose a username for your bot Step 6. BotFather will ask you to choose a username for your bot. This username will be used to create a unique URL that people can use to access your bot. Choose a username that is easy to remember and related to your bot's purpose. At the end of the process, you should have given a bot token by @botFather. You need to save that information, and other information such as the bot name that you chose on the Telegram Bot config file on ./telegram/config.json: { \"bot_name\": \"@whatever_bot_username\", \"name\": \"Your_Bot_Name(what appears on top)\", \"token\": \"Token given by BotFather\" } Configure Ngrok to have a persistant domain Create an account on Ngrok - Step 1. On https://dashboard.ngrok.com/get-started/setup/windows, copy the authtoken and paste it onto the the Telegram Bot config file. same as before, on ./telegram/config.json: - Step 2: Go to \"Domains\" on the left menu, and create a new domain. - Step 3: Copy the domain name and token that you generated and paste it into the Telegram Bot config file. same as before, on ./telegram/config.json: ```json { \"ngrok_token\": \"Token given by Ngrok\", \"domain\": \"Domain given by Ngrok\" } ``` Keep in mind that with the free version you can only have one domain. Configure you RapidAPI sources Create an account on RapidAPI: - Step 1. Go to https://rapidapi.com/ and search for the following APIs: - Google Search API - Step 2. Subscribe to the Medium API: Go to https://rapidapi.com/nishujain199719-vgIfuFHZxVZ/api/medium2 and click on subscribe. - Step 3. Subscribe to the Google API: Go to https://rapidapi.com/rphrp1985/api/google-api31/ and click on subscribe. - Step 4. Copy the key that it shows after subscribing to the API on the API Key Header, ans paste into the config.json inside ./information/sources/rapid/news/config.json and ./information/sources/rapid/medium/config.json: ```json { \"api_key\": \"Key given by RapidAPI\" } ``` Configure your Adobe PDF API Create an account on Adobe PDF - Step 1. Go to the Adobe Developer Console at https://developer.adobe.com/console/home - Step 2. Click on \"Create Project\" - Step 2. Click on add API - Step 3. Select the \"Adobe PDF Services API\" - Step 4. Select OAuth Server to Server as the authentication method - Step 5. Check the Adobe PDF Services box - Step 6. Once created, you should see an OAuth API Key, and a button to generate an access token. Click on the credential. Now you should also see an Organization ID at the bottom - Step 7. Click on generate access token, - Step 9. Copy OAuth API Key, Organization ID and the Access Token into the config.json inside ./pdf/config.json: ```json { \"client_id\": \"OAuth API Key given by Adobe\", \"client_secret\": \"Access Token given by Adobe\", \"service_principal_credentials\": { \"organization_id\": \"Organization ID given by Adobe\" } } ``` Create you LinkedIn page and get the API key Step 1. Go to Linkedin Developers: https://www.linkedin.com/developers/login and login with your Linkedin account. Step 2. Go to \"My Apps\" and click on \"Create App\" Step 3. Fill in all the information required, if you do not have a Linkedin page. You will need to do this as well. Step 4: On you newly created app, go to Auth. Step 5: Copy your Client ID and Client Secret into the ./linkedin/config.json inside ./linkedin/config.json. You might need to click on generate first. ```json { \"client_id\": \"Client ID given by Linkedin\", \"client_secret\": \"Client Secret given by Linkedin\" } ``` - Step 6: Check the OAuth 2.0 Scope permissions. For this project, you need to have these scope permissions (at least): - openid - email - w_member_social - profile Step 7: Copy the persistant URL that we generated before with Ngrok, and paste it into the Authorized Redirect URLs section, adding /callback at the end: https://whatever_domain_you_generated/callback Step 8. Go to the Products Tab, and request access for the following products: Share on Linkedin Sign In with LinkedIn using OpenID Connect After this, if you did not have the necessasry OAuth 2.0 Scope permissions, you should have them now. Step 9. Go to the Team Members Tab, and make sure you appear as Team member. If you do not, add yourself as a team member. Configure your OpenAI API Create an account on OpenAI - Step 1. Go to https://beta.openai.com/ and login with your OpenAI account. - Step 2. Go to \"My Account\" and copy your API key. - Step 3. Paste your API key into the ./llm/langchain_agent/config.json file ```json { \"environment\": { \"OPENAI_API_KEY\": \"apikey\" } } ``` Step 5. If instead of using the OpenAI API, you want to use the OpenAI, you are accessing OpenAI through service, like Azure, you also need to add some more information: ```json { \"environment\": { \"OPENAI_API_KEY\": \"apikey\", \"OPENAI_API_TYPE\": \"type: for example azure\", \"OPENAI_API_VERSION\": \"version: for example: 2023-09-15-preview\", \"OPENAI_API_BASE\": \"your api base endpoint\" } } - Step 6. Configure the OpenAI parameters. If using the OpenAI APIl only the model_name is necesssary. If using it from Open AI studio, you need to add the deployment_id as well. json { \"openai_configs\": { \"deployment_id\": \"\", \"model_name\": \"gpt-4\", \"max_tokens\": 8192 } } ``` Install C++ Build Tools from Visual Studio Torch requires C++ Build Tools from Visual Studio to be installed. For this you can: 1. Install Visual Studio (I installed 2022) 2. Install the C++ Desktop Development Workload 3. Open a command prompt and run: where cl.exe to find the path to the C++ compiler 4. Add the path to the C++ compiler to your PATH environment variable 5. Run C:\\Program Files\\Microsoft Visual Studio xx.x\\VC\\vcvarsall.bat (where xx.x is the version of Visual Studio you installed) to set up the environment variables for the C++ compiler For my architecture and Windows SDK version (gets installed with the C++ Build Tools) I ran \\\"Program Files\"\\\"Microsoft Visual Studio\"\\2022\\Enterprise\\VC\\Auxiliary\\Build\\vcvarsall.bat X64 10.0.22621.0 Install the Windows Service Activate the environment: Open a CMD or Anaconda Prompt as Administrator, next steps will need to run on this session: If using conda: conda activate linkedinassistant Now, run the script configurate.cmd to configure the service. This will create a data folder in C:ProgramData/LinkedinAssistant and copy the config files there. You need to go back to the root folder of the project, and run the script: cd .. configurate.cmd When you run this program as a Windows Service, all logs and config files will be stored in C:ProgramData/LinkedinAssistant folder. Then, you need to create the EXE file with pyinstaller: Some popup error might appear, as long as it is related to torchvison or torchaudio, it is controlled and will be fine, just accept them and let it continue. pyinstaller --hidden-import win32timezone --hidden-import torch --hidden-import torchvision --hidden-import torchaudio --collect-data torch --copy-metadata torch --collect-data torchvision --collect-data langchain --copy-metadata langchain --copy-metadata torchvision --collect-data torchaudio --copy-metadata torchaudio --copy-metadata packaging --copy-metadata safetensors --copy-metadata regex --copy-metadata huggingface-hub --copy-metadata tokenizers --copy-metadata filelock --copy-metadata datasets --copy-metadata numpy --copy-metadata tqdm --copy-metadata requests --copy-metadata pyyaml --clean --noconfirm src\\windows\\service.py This will create the necessary EXE and dependencies in dist folder. After that: .\\dist\\service\\service.exe install If this works, you have done everything correctly. Optionally, you can set the service to start automatically when the computer starts: sc config linkedin_assistant start=delayed-auto Finally, start the service: .\\dist\\service\\service.exe start Author Daniel Cabrera Rodr\u00edguez Github: @dan415 Email: danicr2515@gmail.com Please, do not hesitate to contact if you have any questions or suggestions. License MIT License Copyright (c) 2024 Daniel Cabrera Rodriguez Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Home"},{"location":"index.html#install-the-dependencies","text":"in conda: conda env create -f environment.yml","title":"Install the dependencies"},{"location":"index.html#create-a-telegram-bot-and-get-the-api-key","text":"Step 1. Open the Telegram app on your computer/phone To create a Telegram bot, you'll need to have the Telegram app installed on your computer. If you don't have it already, you can download it from the Telegram website. Step 2. Connect to BotFather BotFather is a bot created by Telegram that allows you to create and manage your own bots. To connect to BotFather, search for \"@BotFather\" in the Telegram app and click on the result to start a conversation. Step 3. Select the New Bot option In the conversation with BotFather, select the \"New Bot\" option to start creating your new bot. BotFather will guide you through the rest of the process. Step 4. Add a bot name Next, BotFather will ask you to provide a name for your bot. Choose a name that accurately reflects the purpose of your bot and is easy to remember. Step 5. Choose a username for your bot Step 6. BotFather will ask you to choose a username for your bot. This username will be used to create a unique URL that people can use to access your bot. Choose a username that is easy to remember and related to your bot's purpose. At the end of the process, you should have given a bot token by @botFather. You need to save that information, and other information such as the bot name that you chose on the Telegram Bot config file on ./telegram/config.json: { \"bot_name\": \"@whatever_bot_username\", \"name\": \"Your_Bot_Name(what appears on top)\", \"token\": \"Token given by BotFather\" }","title":"Create a Telegram Bot and get the API key."},{"location":"index.html#configure-ngrok-to-have-a-persistant-domain","text":"Create an account on Ngrok - Step 1. On https://dashboard.ngrok.com/get-started/setup/windows, copy the authtoken and paste it onto the the Telegram Bot config file. same as before, on ./telegram/config.json: - Step 2: Go to \"Domains\" on the left menu, and create a new domain. - Step 3: Copy the domain name and token that you generated and paste it into the Telegram Bot config file. same as before, on ./telegram/config.json: ```json { \"ngrok_token\": \"Token given by Ngrok\", \"domain\": \"Domain given by Ngrok\" } ``` Keep in mind that with the free version you can only have one domain.","title":"Configure Ngrok to have a persistant domain"},{"location":"index.html#configure-you-rapidapi-sources","text":"Create an account on RapidAPI: - Step 1. Go to https://rapidapi.com/ and search for the following APIs: - Google Search API - Step 2. Subscribe to the Medium API: Go to https://rapidapi.com/nishujain199719-vgIfuFHZxVZ/api/medium2 and click on subscribe. - Step 3. Subscribe to the Google API: Go to https://rapidapi.com/rphrp1985/api/google-api31/ and click on subscribe. - Step 4. Copy the key that it shows after subscribing to the API on the API Key Header, ans paste into the config.json inside ./information/sources/rapid/news/config.json and ./information/sources/rapid/medium/config.json: ```json { \"api_key\": \"Key given by RapidAPI\" } ```","title":"Configure you RapidAPI sources"},{"location":"index.html#configure-your-adobe-pdf-api","text":"Create an account on Adobe PDF - Step 1. Go to the Adobe Developer Console at https://developer.adobe.com/console/home - Step 2. Click on \"Create Project\" - Step 2. Click on add API - Step 3. Select the \"Adobe PDF Services API\" - Step 4. Select OAuth Server to Server as the authentication method - Step 5. Check the Adobe PDF Services box - Step 6. Once created, you should see an OAuth API Key, and a button to generate an access token. Click on the credential. Now you should also see an Organization ID at the bottom - Step 7. Click on generate access token, - Step 9. Copy OAuth API Key, Organization ID and the Access Token into the config.json inside ./pdf/config.json: ```json { \"client_id\": \"OAuth API Key given by Adobe\", \"client_secret\": \"Access Token given by Adobe\", \"service_principal_credentials\": { \"organization_id\": \"Organization ID given by Adobe\" } } ```","title":"Configure your Adobe PDF API"},{"location":"index.html#create-you-linkedin-page-and-get-the-api-key","text":"Step 1. Go to Linkedin Developers: https://www.linkedin.com/developers/login and login with your Linkedin account. Step 2. Go to \"My Apps\" and click on \"Create App\" Step 3. Fill in all the information required, if you do not have a Linkedin page. You will need to do this as well. Step 4: On you newly created app, go to Auth. Step 5: Copy your Client ID and Client Secret into the ./linkedin/config.json inside ./linkedin/config.json. You might need to click on generate first. ```json { \"client_id\": \"Client ID given by Linkedin\", \"client_secret\": \"Client Secret given by Linkedin\" } ``` - Step 6: Check the OAuth 2.0 Scope permissions. For this project, you need to have these scope permissions (at least): - openid - email - w_member_social - profile Step 7: Copy the persistant URL that we generated before with Ngrok, and paste it into the Authorized Redirect URLs section, adding /callback at the end: https://whatever_domain_you_generated/callback Step 8. Go to the Products Tab, and request access for the following products: Share on Linkedin Sign In with LinkedIn using OpenID Connect After this, if you did not have the necessasry OAuth 2.0 Scope permissions, you should have them now. Step 9. Go to the Team Members Tab, and make sure you appear as Team member. If you do not, add yourself as a team member.","title":"Create you LinkedIn page and get the API key"},{"location":"index.html#configure-your-openai-api","text":"Create an account on OpenAI - Step 1. Go to https://beta.openai.com/ and login with your OpenAI account. - Step 2. Go to \"My Account\" and copy your API key. - Step 3. Paste your API key into the ./llm/langchain_agent/config.json file ```json { \"environment\": { \"OPENAI_API_KEY\": \"apikey\" } } ``` Step 5. If instead of using the OpenAI API, you want to use the OpenAI, you are accessing OpenAI through service, like Azure, you also need to add some more information: ```json { \"environment\": { \"OPENAI_API_KEY\": \"apikey\", \"OPENAI_API_TYPE\": \"type: for example azure\", \"OPENAI_API_VERSION\": \"version: for example: 2023-09-15-preview\", \"OPENAI_API_BASE\": \"your api base endpoint\" } } - Step 6. Configure the OpenAI parameters. If using the OpenAI APIl only the model_name is necesssary. If using it from Open AI studio, you need to add the deployment_id as well. json { \"openai_configs\": { \"deployment_id\": \"\", \"model_name\": \"gpt-4\", \"max_tokens\": 8192 } } ```","title":"Configure your OpenAI API"},{"location":"index.html#install-c-build-tools-from-visual-studio","text":"Torch requires C++ Build Tools from Visual Studio to be installed. For this you can: 1. Install Visual Studio (I installed 2022) 2. Install the C++ Desktop Development Workload 3. Open a command prompt and run: where cl.exe to find the path to the C++ compiler 4. Add the path to the C++ compiler to your PATH environment variable 5. Run C:\\Program Files\\Microsoft Visual Studio xx.x\\VC\\vcvarsall.bat (where xx.x is the version of Visual Studio you installed) to set up the environment variables for the C++ compiler For my architecture and Windows SDK version (gets installed with the C++ Build Tools) I ran \\\"Program Files\"\\\"Microsoft Visual Studio\"\\2022\\Enterprise\\VC\\Auxiliary\\Build\\vcvarsall.bat X64 10.0.22621.0","title":"Install C++ Build Tools from Visual Studio"},{"location":"index.html#install-the-windows-service","text":"Activate the environment: Open a CMD or Anaconda Prompt as Administrator, next steps will need to run on this session: If using conda: conda activate linkedinassistant Now, run the script configurate.cmd to configure the service. This will create a data folder in C:ProgramData/LinkedinAssistant and copy the config files there. You need to go back to the root folder of the project, and run the script: cd .. configurate.cmd When you run this program as a Windows Service, all logs and config files will be stored in C:ProgramData/LinkedinAssistant folder. Then, you need to create the EXE file with pyinstaller: Some popup error might appear, as long as it is related to torchvison or torchaudio, it is controlled and will be fine, just accept them and let it continue. pyinstaller --hidden-import win32timezone --hidden-import torch --hidden-import torchvision --hidden-import torchaudio --collect-data torch --copy-metadata torch --collect-data torchvision --collect-data langchain --copy-metadata langchain --copy-metadata torchvision --collect-data torchaudio --copy-metadata torchaudio --copy-metadata packaging --copy-metadata safetensors --copy-metadata regex --copy-metadata huggingface-hub --copy-metadata tokenizers --copy-metadata filelock --copy-metadata datasets --copy-metadata numpy --copy-metadata tqdm --copy-metadata requests --copy-metadata pyyaml --clean --noconfirm src\\windows\\service.py This will create the necessary EXE and dependencies in dist folder. After that: .\\dist\\service\\service.exe install If this works, you have done everything correctly. Optionally, you can set the service to start automatically when the computer starts: sc config linkedin_assistant start=delayed-auto Finally, start the service: .\\dist\\service\\service.exe start","title":"Install the Windows Service"},{"location":"information/rationale.html","text":"Information Searching Component This module is responsible for searching for information using different sources. Sources The following sources are currently supported: Arxiv Google News Medium Inputted PDFS (from a directory) The sources are handled by Sources Handler class. I intent to extend the sources to many more, and it is very easy to add more sources, thanks to the modular design of the program. Configuration The configuration file is located in the config.json file. The following parameters are available: active_sources : A list of strings containing the names of the sources to be loaded. The names must match the Enum string values of the InformationSource class. execution_period : Time on days between executions. This is used by the Source Handler class. sleep_time : Number of execution periods to wait between searches. Only used by Source Handler class. process_sleep_time : The time in seconds to wait between processes. Only used by Publications Handler class. last_run_time : The last time the program was run. This is used to calculate the time elapsed since the last run, and to determine if the program should run again. one_by_one : If set to true, the program will run the sources one by one, instead of running them concurrently. This is useful for debugging purposes. active : If set to false, the program will not run. This is useful for debugging purposes. publications_directory : The directory where the publications are stored. This is used by the manual source. publications_pending_approval_directory : The directory where the publications pending approval are stored. This is used by the manual source.","title":"About"},{"location":"information/publications_handler/rationale.html","text":"Publications Handler This class is responsible for handling the publications. It is responsible for the following: Walk the publications_directory Process the publication, generating a publication idea using the LLM Module. Save the pickled langchan memory (that stores the gpt response and conversation history) inside the pending approval directory.","title":"Publications Handler"},{"location":"information/sources/arxiv/rationale.html","text":"Arxiv This Content Search Engine is based on the Arxiv API . The API is used to retrieve the information of the papers, and the search engine is used to index the information and provide a search interface. It searches papers in the Arxiv API, filtering by by the topics specified in config, including results only from last week and with a set maximum of results. The Arxiv API returns the papers' metadata in XML format, which is then converted to json. From there, we also download the PDF bytes of the paper and pass it directly to the Adobe PDF Services API. After that, we add the paper content to the json, and index using the ColBert model. Then the model is queried with the colbert queries in order to retrieve the meaninful information that we want to use for generating the publication later on. Configuration The configuration file is located in the information/sources/arxiv folder, and is named config.json . It contains the following parametes: max_results : The maximum number of results to retrieve from the API. url : The URL of the Arxiv API. minimum_length : The minimum length of the contents. Papers with content less than this number of characters are filtered out paragraph_min_length : The minimum length of the paragraphs. Paragraphs with content less than this number of characters are filtered out topics : A list of topics to search for. The topics are used to filter the results from the API. Only papers that contain at least one of the topics are retrieved. Some of these topic codes are: cat:cs.AI: Artificial Intelligence cat:cs.CL: Computation and Language cat:cs.GT: Computer Science and Game Theory\u00e7 cat:cs.CV: Computer Vision and Pattern Recognition cat:cs.ET: Emerging Technologies cat:cs.IR: Information Retrieval cat:cs.LG: Machine Learning cat:cs.NE: Neural and Evolutionary Computing cat:cs.PL: Programming Languages cat:cs.RO: Robotics period : The period of time to search for in days. colbert_max_total : The maximum number of results to retrieve from the API when using the ColBERT model. colbert_queries : A list of queries to search for when using the ColBERT model. The queries are used to make the searches against the indexes created by ColBert","title":"Arxiv"},{"location":"information/sources/gmail/rationale.html","text":"GMail The GMail source has not been implemented yet :)","title":"Rationale"},{"location":"information/sources/manual_pdfs/rationale.html","text":"Manual PDF Input This Content Search Engine is used to create posts from papers that you want to force the agent to process. The way to do this would be to add the PDF inside the designated input folder. Then, when the Sources Handler runs, it will detect the PDF and will create a post from it. The post will be stored in the pending approval folder. The agent reads from the input folder, and pass the papers directly to the Adobe PDF Services API. After that, we add the paper content to the json, and index using the ColBert model. Then the model is queried with the colbert queries in order to retrieve the meaninful information that we want to use for generating the publication later on. The sources handler will save the post in the pending approval folder, and will move the actual pdf from the input directory to the output directory. This is just to keep the input directory clean, and to avoid processing the same pdfs over and over again, and not to delete the pdfs. Also, it serves as a method to keep track of the pdfs that have been processed. Configuration The configuration file is located in the information/sources/manual_pdfs folder, and is named config.json . It contains the following parametes: minimum_length : The minimum length of the contents. Papers with content less than this number of characters are filtered out paragraph_min_length : The minimum length of the paragraphs. Paragraphs with content less than this number of characters are filtered out colbert_max_total : The maximum number of results to retrieve from the API when using the ColBERT model. colbert_queries : A list of queries to search for when using the ColBERT model. The queries are used to make the searches against the indexes created by ColBert input_directory : The directory where the PDFs to be processed are located. output_directory : The directory where the PDFs that have been processed are moved to.","title":"Manual PDF Input"},{"location":"information/sources/rapid/medium/rationale.html","text":"Medium This Content Search Engine is based on the Rapid API . This Content Search Engine is used to retrieve information from Medium articles. Configuration The configuration file is located in the information/sources/rapid/medium folder, and is named config.json . It contains the following parameters: api_key : The API key for the Rapid API source. limit : The maximum number of results to retrieve from the API. period : The period of time to search for in days. url : The URL of the Rapid API source. host : The host of the Rapid API source. max_results : The maximum number of results to retrieve from the API. minimum_length : The minimum length of the contents. Papers with content less than this number of characters are filtered out count_requests : An integer counter that keeps track of the number of requests made to the API. This is used to keep track of the number of requests made to the API, and to disable the source when the limit is reached. topics : A list of topics to search for. Every topic is a \"normal\" passed to medium's search engine.","title":"Medium"},{"location":"information/sources/rapid/news/rationale.html","text":"Google News This Content Search Engine is based on the Rapid API . This Content Search Engine is used to retrieve information from Google News. Configuration The configuration file is located in the information/sources/rapid/news folder, and is named config.json . It contains the following parameters: api_key : The API key for the Rapid API source. limit : The maximum number of results to retrieve from the API. period : The period of time to search for in days. url : The URL of the Rapid API source. host : The host of the Rapid API source. max_results : The maximum number of results to retrieve from the API. minimum_length : The minimum length of the contents. Papers with content less than this number of characters are filtered out count_requests : An integer counter that keeps track of the number of requests made to the API. This is used to keep track of the number of requests made to the API, and to disable the source when the limit is reached. topics : A list of topics to search for. Every topic is a \"normal\" passed to medium's search engine.","title":"Rationale"},{"location":"information/sources_handler/rationale.html","text":"Sources Handler The Sources Handler class is responsible for handling the sources. It is responsible for the following: Run search engines for each source asynchronously and concurrently. Sleep for a specified amount of time between searches. Supported information sources are loaded from the config.json file. Sources defined in the confif file need to exist as Enum Items of InformationSource class inside information/sources/information_source.py. The Enum string value is used in order to load the sources from: { \"active_sources\": [ \"medium\", \"arxiv\", \"google_news\", \"manual\" ] } Changing this list will change the sources that are loaded. The output from sources may vary from source to source, but retrieved information is always stored as json inside the publications_directory. Once they get processed by the Publications handler, they get moved to the pending approval directory.","title":"Sources Handler"},{"location":"linkedin/rationale.html","text":"LinkedIn This module is used to integrate with LinkedIn. It is used to authenticate the user and to give access to the LinkedIn API to post content. Authentication This module exposes two REST methods to authenticate the user with LinkedIn. The first one is used to get the URL to redirect the user to, and the second one is used to get the access token after the user has been redirected back to the application. The authentication process is as follows: 1. The user tries to publish content to LinkedIn. 2. The application checks if the user is authenticated. If not, it redirects the user to the authentication URL, exposed on \"/\". This sends the auth request to LinkedIn using the client_id and secret of the application. 3. After the user is redirected to the LinkedIn Authentication page, the user needs to log in with an account that is member of the page. 4. After the user logs in, LinkedIn redirects the user back to the application, with the access token as a parameter at \"/callback\". 5. The Callback saves the access token in the LinkedIn config file, and then the user can finally prompt to publish again, this time succesfully. Configuration The configuration file is located in the information/sources/linkedin folder, and is named config.json . It contains the following parameters: client_id : The client ID of the application. This is used to authenticate the user with LinkedIn. client_secret : The client secret of the application. This is used to authenticate the user with LinkedIn. redirect_uri : The redirect URI of the application. This is used to authenticate the user with LinkedIn. access_token : The access token of the user. This is used to authenticate the user with LinkedIn. linkedin_id : The LinkedIn ID of the page to post to. This is used to authenticate the user with LinkedIn. footer : The footer to add to the post. This is used to add a footer to the post. I use it to specify that the post was posted automatically by an AI","title":"Rationale"},{"location":"llm/rationale.html","text":"LLM Components This section contains the documentation for the LLM module. The LLM module is composed of two components: The langchain agent, which is the main component, in charge of actually using ChatGPT in order to generate the content. The ColBERT agent, which is in charge of the information retrieval part of the LLM module. In following updates, the structure of the LLM module will be generalized in order to add more information retrieval methods, and more conversational agents.","title":"LLM Components"},{"location":"llm/colbert/rationale.html","text":"ColBERT Agent ColBERT is a fast and accurate retrieval model, enabling scalable BERT-based search over large text collections in tens of milliseconds. Our ColBERT agent is based on colBERT V2 , version 0.2.0. They already include a very detailed documentation, so we will not go into detail here. When installing this program as a service, Deberta V2 by default will not compile as TorchScript requires the source code to be available. For this reason, I have added a patch in order to switch @torch.jit.script for @torch.jit._script_if_tracing. The model will supposedly be slower, but it will work. If you want to use the model in TorchScript, you can remove the patch from the code. On src/llm/ColBERT/modelling/hf_colbert.py: def script_method(fn, _rcb=None): return fn def script(obj, optimize=True, _frames_up=0, _rcb=None): return obj import torch.jit script_method1 = torch.jit.script_method script1 = torch.jit.script_if_tracing torch.jit.script_method = script_method torch.jit.script = script ColBERT Agent Configuration The configuration file is located in the llm/ColBERT folder, and is named config.json . It contains the following parameters: k : The number of documents to retrieve from the index. queries : The queries to use for the query-document retrieval. This is a list of strings. kmeans_iters : The number of iterations to use for the k-means clustering algorithm. document_max_length : The maximum length of the document to index.","title":"ColBERT"},{"location":"llm/langchain_agent/rationale.html","text":"Langchain Agent The Langchain agent is based on a langchain Chain for OpenAI models (which will be extended for more models in the future). It is as well based on two components: - A chat prompt template consisting on a system prompt and a user prompt. - A memory, which is injected into the user prompt and stores the historical context of the conversation. Chat Prompt Template As previously said, the chat prompt template is composed of two parts: The system prompt: The system instructs the agent to encompass creating engaging posts on relevant news, papers, and articles identified by an automated ideas retrieval pipeline. Tasked with an adept writing style that consistently captivates readers, the agent receives the pipeline's output, generate initial posts, and subsequently undergoes a meticulous review process led by the Data Scientist. This entails a collaborative loop where the Data Scientist may either approve, reject, or request changes to the post. It is said to it that its proficiency lies in not only crafting attention-grabbing content but also excelling at summarizing key article points concisely. Moreover, it is told it specializes in elucidating innovative ideas, particularly those related to state-of-the-art techniques. As per the given prompt, it is poised to dive into the creation of a post centered around the specified article, applying its unique style and expertise to deliver compelling and informative content. All this in the context of being an assistant for a Data Scientist role. User prompt: The user prompt only contains two placeholders: \"chat_history\": This placeholder is replaced by the memory of the agent, which is the historical context of the conversation. \"input\": This placeholder is replaced with the user agent message. However, when the publications handler goes to generate the first message, which would be the initial post generation, a custom prompt template is built from the keys of the json representing the post. Remember that the post is a json at this point. For example: { \"title\": \"This is the title\", \"author\": \"This is the author\", \"date\": \"This is the date\", \"description\": \"This is the description\", \"content\": \"This is the content\" } Langchain Agent Configuration The configuration file is located in the llm/langchain_agent folder, and is named config.json . It contains the following parameters: environment : OPENAI_API_KEY : The OpenAI API key to use. OPENAI_API_TYPE : The OpenAI API type to use. See the OpenAI API documentation for more information. OPENAI_API_VERSION : The OpenAI API version to use, e.g: 2020-05-03 is supported. OPENAI_API_BASE : The OpenAI API base URL to use, if using Azure Open AI studio, this should be your deployment url openai_configs : This is a dictionary with whatever the OpenAI class needs to initialize. This is passed directly to the OpenAI class. e.g: model temperature, penalty, etc. max_conversation_length : The maximum length of the conversation. This is used to limit the memory of the agent at n previous messages. system_prompt_template : The system prompt template to use. This is the system prompt described in the previous section. This prompt can be safely changed to human_message_template : The human message template to use. This is the user prompt described in the previous section. This should not be changed.","title":"Langchain Agent"},{"location":"main/rationale.html","text":"Main Modules This module only contains the main script that is run in order to run the different services that form the application. It creates the services as asyncio tasks and runs them in parallel. Tasks The main tasks that are run by this module are the following: Auth server: This task runs the authentication flask server that is used to authenticate users and to generate the access tokens that are used to publish on LinkedIn. Bot Agent: The Telegram Bot Publications Handler: This task is responsible for publishing the publications on LinkedIn Sources Handler: This task is responsible for retrieving the publications from the different sources","title":"Main Component"},{"location":"pdf/rationale.html","text":"Adobe PDF module This module is used to get the pdf text file. The reason why I use this API instead of some free library is that this is one of the more accurate libraries I have seen at retrieving the chapters/sections of a pdf file. It is not perfect, but it is the best I have seen so far. Note that a PDF is unstructured data, so it is not possible to get the paragraphs of a PDF in one single way What I do is call the API with the pdf bytes. The API is going to download the results on a file, that I do not want to keep, so I create a TemporaryDirectory and extract the results there. Then I read the json that's in the Zip and return the chapters. The Temp dir is discarded so I do not need to worry about deleting the files. Note that this API has a limit per month, and any attempts will fail if the limit is reached. Configuration The configuration is stored in a json file named config.json inside ./pdf. It has the following format: { \"client_credentials\": { \"client_id\": \"\", \"client_secret\": \"\" }, \"service_principal_credentials\": { \"organization_id\": \"\" } } These credentials are obtained from the Adobe Developer Console. More information on the Installation section. Extraction The extraction process is as follows: I iterate through the elements of the generated json with the elements of the pdf. For each element with regex pattern //Document/H1[\\[*\\]]* ````` I consider to be a chapter title. For each chapter title, I iterate through the elements, obtaining elements with either paragraph pattern: ```regex //Document/P(\\[[\\d]*\\])?(?:/ParagraphSpan(\\[\\d]*\\])?)?$ or bullet point pattern: //Document/L*LBody$ and append them to the chapter if they are not of length 0.","title":"PDF Handler"},{"location":"resources/rationale.html","text":"Resources For this program to work Tasks The main tasks that are run by this module are the following: Auth server: This task runs the authentication flask server that is used to authenticate users and to generate the access tokens that are used to publish on LinkedIn. Bot Agent: The Telegram Bot Publications Handler: This task is responsible for publishing the publications on LinkedIn Sources Handler: This task is responsible for retrieving the publications from the different sources","title":"Rationale"},{"location":"telegram/rationale.html","text":"Telegram Bot The idea of this bot is that it will allow to interact with the system from a Telegram client. The bot will allow to chat with the LLM conversation agent, receive publication suggestions, ask the agent to change the publication drafts, discard drafts, and publish them. The bot is based on the Origami Telegram Bot package, used for building Telegram bots in Python. In order to be able to circle through conversation threads, the memory pickle of the LLM agents is actually the object that is stored on the pending for approval directory. These are loaded and maintained in a circular list of suggestions. The bot will then allow to move forward and backwards through the list of suggestions, and to approve, discard them or change them. Suggestion Pool The suggestion pool is a list of suggestions that are stored in the pending for approval directory. Each suggestion is composed by a memory pickle (path to it) and an index, specifying the index of the suggestion in the list of suggestions. This two fields are stored in a JSON file called config.json inside telegram/suggestions. It has the following format: { \"pool\": [ { \"id\": 0, \"path\": \"absolute_path.pkl\" }, { \"id\": 1, \"path\": \"absolute_path.pkl\" } ], \"current\": { \"id\": 5, \"path\": \"absolute_path.pkl\" } } The suggestions pool can be iterated through, and it stores the current suggestion index in the current field. The current field also updates when the user uses the next and previous commands. The suggestion pool also provide a select method, which allows to select a suggestion by its index. Please refer to the source code of pool.py for more detailed information about the methods for now. Algorithm The algorithm is as follows: While True: 1. Check if the bot has just published. If so, wait for suggestion_period days. 2. Check if suggestions are blocked. If so, wait for 5 minutes in order to check again 3. If suggestions are not blocked, update the suggestions and check if there are suggestions. 4. If there are suggestions, send the current suggestion and block suggestions. Then the flow of the program is carried by the interaction with the user via Telegram. Bot State This class is used to store the state of the bot. It is used to store the chat id, the current suggestion, and the suggestions are blocked or not. For every publication (suggestion) there needs to be a conversation thread. Via Bot commands I can change the conversation thread, so I can finish \"tuning\" the publication and then publish it. As said on the Algorithm section, suggestions can be blocked. This is done in order to avoid the bot to send suggestions when the user is already interacting with the bot. The bot will send a suggestion, and then block suggestions until the user has finished interacting with the bot. This is done by setting the suggestions_blocked field to True. When the user finishes interacting with the bot, the suggestions_blocked field is set to False, and the bot will send a new suggestion if there is one available. Also, if the sending of suggestions fails, suggestions get blocked in order to avoid the bot sending failing continuously. The Bot is now designed to respond to one person only. It obviosly could respond to whoever sent messages to it, but in order to send suggestions (which do not require interaction with the user) it needs to know the chat id of the user. This is why the bot is designed to respond to one person only. This is the purpose of the chat_id field. The suggestions are in order, so I can go to the next or previous suggestion. I can also select a suggestion by index. Also, this state also saves information about when to automatically make suggestions, which would be when no suggestion is selected and after some time after a publication is made. The stateful decorator is used to update the config file after a function is called, so the file is always updated with the object state. Bot Commands /start : Starts the bot. It will send a welcome message and a suggestion if there is one available. /next : Sends the next suggestion (and changes the thread of conversation) if there is one available. /previous : Sends the previous suggestion (and changes the thread of conversation) if there is one available. /select {index} : Selects a suggestion by index. It will send the selected suggestion and change the thread of conversation. /list : Lists the suggestions in the pool with their index. /publish : Publishes the current suggestion. If not authenticated, it will send a message with the link to the authentication page. /allow : Allows the bot to send suggestions. /stop : Blocks the bot from sending suggestions. /healthcheck : Sends a message to the user (to see if the bot is alive). /clear : Discards the current suggestion. /update : Updates the suggestions pool. It reloads them from the pending for approval directory. /current : Sends the current suggestion. This is what will get published if the user uses the /publish command.","title":"Telegram Bot"},{"location":"todo/rationale.html","text":"Next steps I would like to implement the following features: [ ] Implement Youtube as a source of information, adding support with Whisper. [ ] Implement a way to add new sources of information from Telegram. For example: send it pdfs that can be then added to the manual pdf input folder. [ ] Add a parent class for ColBERT called RAG, and implement other methods of information Retrieval. [ ] Same with the langchain agent for other Huggingface models and Replicate. [ ] Integrate with Dall-E to generate images to accompany the post [ ] Add support to program posts in advance. [ ] Dockerize the project. [ ] Explain how to install and use the project as a service in Linux [ ] Expand to other Social Media platforms. All of these are ideas that I have in mind, but I am open to suggestions and contributions. However, I am busy, so I cannot guarantee that I will implement them in the near future. If you want to contribute, or have any ideas, please do not hesitate to contact me.","title":"Next Steps"},{"location":"windows/rationale.html","text":"Windows Service This module contains the code that is used to run the application as a Windows service. It uses the code on the main module to run the different services, and it also contains the code that is used to install the service, and to respond to the different signals that are sent to the service.","title":"Windows Service"}]}