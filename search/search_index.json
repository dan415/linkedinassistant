{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"LinkedIn Gen AI Driven Posting Creator Assistant This project was created to simplify the process of sharing posts about fascinating articles and papers that surface every week. The program generates drafts for these posts, allowing the user to focus only on reviewing and publishing. Components Overview This program comprises the following components: - Core : Implements shared utilities like configuration management and Vault secrets handling. - Information : Handles raw information scraping and generates initial publication drafts. - Telegram : Acts as the user interface via a Telegram bot. - LinkedIn : Manages LinkedIn API interactions and OAuth server functionalities. Installation Step 1: Set Up HashiCorp Secrets Vault Step 1: Set Up HCP Account Sign Up/Login : Visit the HashiCorp Cloud Platform and log in or create an account. Billing : Add billing details. Step 2: Create an HCP Organization Navigate to Organizations and create one. Save the Organization Name as HCP_ORGANIZATION . Step 3: Create an HCP Project In your organization, navigate to Projects and create one. Save the Project Name as HCP_PROJECT . Step 4: Set Up HCP Vault Go to HashiCorp Vault and create a Vault cluster. Save the cluster details. Step 5: Create an Application in HCP Vault Go to Access Control and create an application. Save the App Name as HCP_APP . Generate a Client ID and Client Secret , saving them as HCP_CLIENT_ID and HCP_CLIENT_SECRET . Environment Variables Create a .env file at the root project level: HCP_ORGANIZATION=your_organization HCP_PROJECT=your_project HCP_APP=your_app HCP_CLIENT_ID=your_client_id HCP_CLIENT_SECRET=your_client_secret Note: This environment variables will only be needed up until installation is completed. After that, they will have been loaded into the system's keyring or into Dockerized environment Step 2: Set Up MongoDB Atlas Step 1: Create a MongoDB Atlas Account Visit MongoDB Atlas and sign up. Deploy a free M0 Sandbox cluster. Obtain the connection string and save it as MONGO_URI . All necessary collections and documents will be created at start time. Necessary configs reside inside res/json/default_configs.json . Step 3: Set Up Backblaze B2 File Storage Sign up at Backblaze . Create a B2 Cloud Storage bucket named linkedin-assistant . Generate application keys and save them as BLACBLAZE_API_KEY and BLACBLAZE_KEY_ID . Step 4: Set Up Telegram Bot Connect to BotFather . Create a bot and save the token as TELEGRAM_BOT_TOKEN . Store the bot configuration in MongoDB: { \"bot_name\": \"@your_bot_username\", \"name\": \"Your Bot Name\", \"token\": \"your_bot_token\" } Step 5: Configure LinkedIn API Step 1: Go to Linkedin Developers: https://www.linkedin.com/developers/login and login with your Linkedin account. Step 2. Go to \"My Apps\" and click on \"Create App\" Step 3. Fill in all the information required, if you do not have a Linkedin page. You will need to do this as well. Step 4: On you newly created app, go to Auth. Step 5: Save these in your Vault as LINKEDIN_CLIENT_ID and LINKEDIN_CLIENT_SECRET . Check the OAuth 2.0 Scope permissions. For this project, you need to have these scope permissions (at least): openid email w_member_social profile Step 6: Copy the persistant URL that we generated before with Ngrok without the HTTP schema and set it in your vault as NGROK_DOMAIN Step 8. Go to the Products Tab, and request access for the following products: Share on Linkedin Sign In with LinkedIn using OpenID Connect After this, if you did not have the necessasry OAuth 2.0 Scope permissions, you should have them now. - Step 9. Go to the Team Members Tab, and make sure you appear as Team member. If you do not, add yourself as a team member. Step 6: Set Up Rapid API (Optional, if want to use Rapid API sources) Create an account on RapidAPI: - Step 1. Go to https://rapidapi.com/ and search for the following APIs: - Google Search API - Step 2. Subscribe to the Medium API: Go to https://rapidapi.com/nishujain199719-vgIfuFHZxVZ/api/medium2 and click on subscribe. - Step 3. Subscribe to the Google API: Go to https://rapidapi.com/rphrp1985/api/google-api31/ and click on subscribe. - Step 4: Subscribe to Youtube Transcribe API: Go to https://rapidapi.com/ and click subscribe - Step 5. Copy the key that it shows after subscribing to the API on the API Key Header, and set it in you vault as RAPID_API_KEY Step 8: Obtain Youtube API Key from Google Cloud Platform (Optional, if want to use Youtube as source) Step 1: Set Up a Google Cloud Project Log in to Google Cloud Console Go to Google Cloud Console. Create a New Project Click on the \"Select a project\" dropdown in the top navigation bar. Click New Project. Enter a Project Name, choose your Billing Account (if prompted), and click Create. Step 2: Enable YouTube Data API v3 Navigate to the APIs & Services Dashboard In the left-hand menu, click APIs & Services > Library. Search for YouTube Data API v3 In the search bar, type YouTube Data API v3. Click on the API from the search results. Enable the API Click the Enable button to activate the YouTube Data API for your project. Step 3: Create API Credentials Go to the Credentials Page In the left-hand menu, click APIs & Services > Credentials. Click \"Create Credentials\" In the top toolbar, click the \"Create Credentials\" button. Select API Key from the dropdown. Copy the API Key After creation, a pop-up will display your new API key. Click the Copy icon and save the key somewhere secure. Step 4: Restrict Your API Key (Optional) To ensure security, restrict how and where your API key can be used: Edit API Key Restrictions On the Credentials page, click the Edit icon next to your API key. Set Application Restrictions Under Application restrictions, select HTTP referrers (web sites). Add the URL of your website or application. Set API Restrictions Under API restrictions, select Restrict key. Select the YouTube Data API v3 from the dropdown. Click Save. Step 9: Set up desired LLM providers Available supported providers are: OpenAI Groq Google Gen AI Deepseek You can obtain the API keys for these providers and set them in your Vault as OPENAI_API_KEY , GROQ_API_KEY , GOOGLE_GEN_AI_API_KEY , and DEEPSEEK_API_KEY respectively. Note: OpenAI is the only image model provider available at the moment. Therefore, it is mandatory to set the OPENAI_API_KEY in your Vault, if you want to use the image generation tool . Step 10: Install C++ Build Tools from Visual Studio (Only if installing as Windows Service) Torch requires C++ Build Tools from Visual Studio to be installed. For this you can: 1. Install Visual Studio (I installed 2022) 2. Install the C++ Desktop Development Workload 3. Open a command prompt and run: where cl.exe to find the path to the C++ compiler 4. Add the path to the C++ compiler to your PATH environment variable 5. Run C:\\Program Files\\Microsoft Visual Studio xx.x\\VC\\vcvarsall.bat (where xx.x is the version of Visual Studio you installed) to set up the environment variables for the C++ compiler For my architecture and Windows SDK version (gets installed with the C++ Build Tools) I ran \\\"Program Files\"\\\"Microsoft Visual Studio\"\\2022\\Enterprise\\VC\\Auxiliary\\Build\\vcvarsall.bat X64 10.0.22621.0 Step 11: Install the service If using Windows: Open root project folder on CMD Terminal with Admin Privileges and run Run .\\install.bat (If you want to rebuild the EXE file run with argument --rebuild) The program will ask for your user password in order to install the service with your account (It will not validate it but installation will fail at the end if incorrectly provided) If using Unix, on terminal, run: sudo ./install.sh Author Daniel Cabrera Rodr\u00edguez GitHub : @dan415 Email : danicr2515@gmail.com Feel free to reach out with questions or suggestions! License MIT License \u00a9 2024 Daniel Cabrera Rodr\u00edguez Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"Home"},{"location":"index.html#linkedin-gen-ai-driven-posting-creator-assistant","text":"This project was created to simplify the process of sharing posts about fascinating articles and papers that surface every week. The program generates drafts for these posts, allowing the user to focus only on reviewing and publishing.","title":"LinkedIn Gen AI Driven Posting Creator Assistant"},{"location":"index.html#components-overview","text":"This program comprises the following components: - Core : Implements shared utilities like configuration management and Vault secrets handling. - Information : Handles raw information scraping and generates initial publication drafts. - Telegram : Acts as the user interface via a Telegram bot. - LinkedIn : Manages LinkedIn API interactions and OAuth server functionalities.","title":"Components Overview"},{"location":"index.html#installation","text":"","title":"Installation"},{"location":"index.html#step-1-set-up-hashicorp-secrets-vault","text":"","title":"Step 1: Set Up HashiCorp Secrets Vault"},{"location":"index.html#step-1-set-up-hcp-account","text":"Sign Up/Login : Visit the HashiCorp Cloud Platform and log in or create an account. Billing : Add billing details.","title":"Step 1: Set Up HCP Account"},{"location":"index.html#step-2-create-an-hcp-organization","text":"Navigate to Organizations and create one. Save the Organization Name as HCP_ORGANIZATION .","title":"Step 2: Create an HCP Organization"},{"location":"index.html#step-3-create-an-hcp-project","text":"In your organization, navigate to Projects and create one. Save the Project Name as HCP_PROJECT .","title":"Step 3: Create an HCP Project"},{"location":"index.html#step-4-set-up-hcp-vault","text":"Go to HashiCorp Vault and create a Vault cluster. Save the cluster details.","title":"Step 4: Set Up HCP Vault"},{"location":"index.html#step-5-create-an-application-in-hcp-vault","text":"Go to Access Control and create an application. Save the App Name as HCP_APP . Generate a Client ID and Client Secret , saving them as HCP_CLIENT_ID and HCP_CLIENT_SECRET .","title":"Step 5: Create an Application in HCP Vault"},{"location":"index.html#environment-variables","text":"Create a .env file at the root project level: HCP_ORGANIZATION=your_organization HCP_PROJECT=your_project HCP_APP=your_app HCP_CLIENT_ID=your_client_id HCP_CLIENT_SECRET=your_client_secret Note: This environment variables will only be needed up until installation is completed. After that, they will have been loaded into the system's keyring or into Dockerized environment","title":"Environment Variables"},{"location":"index.html#step-2-set-up-mongodb-atlas","text":"","title":"Step 2: Set Up MongoDB Atlas"},{"location":"index.html#step-1-create-a-mongodb-atlas-account","text":"Visit MongoDB Atlas and sign up. Deploy a free M0 Sandbox cluster. Obtain the connection string and save it as MONGO_URI . All necessary collections and documents will be created at start time. Necessary configs reside inside res/json/default_configs.json .","title":"Step 1: Create a MongoDB Atlas Account"},{"location":"index.html#step-3-set-up-backblaze-b2-file-storage","text":"Sign up at Backblaze . Create a B2 Cloud Storage bucket named linkedin-assistant . Generate application keys and save them as BLACBLAZE_API_KEY and BLACBLAZE_KEY_ID .","title":"Step 3: Set Up Backblaze B2 File Storage"},{"location":"index.html#step-4-set-up-telegram-bot","text":"Connect to BotFather . Create a bot and save the token as TELEGRAM_BOT_TOKEN . Store the bot configuration in MongoDB: { \"bot_name\": \"@your_bot_username\", \"name\": \"Your Bot Name\", \"token\": \"your_bot_token\" }","title":"Step 4: Set Up Telegram Bot"},{"location":"index.html#step-5-configure-linkedin-api","text":"Step 1: Go to Linkedin Developers: https://www.linkedin.com/developers/login and login with your Linkedin account. Step 2. Go to \"My Apps\" and click on \"Create App\" Step 3. Fill in all the information required, if you do not have a Linkedin page. You will need to do this as well. Step 4: On you newly created app, go to Auth. Step 5: Save these in your Vault as LINKEDIN_CLIENT_ID and LINKEDIN_CLIENT_SECRET . Check the OAuth 2.0 Scope permissions. For this project, you need to have these scope permissions (at least): openid email w_member_social profile Step 6: Copy the persistant URL that we generated before with Ngrok without the HTTP schema and set it in your vault as NGROK_DOMAIN Step 8. Go to the Products Tab, and request access for the following products: Share on Linkedin Sign In with LinkedIn using OpenID Connect After this, if you did not have the necessasry OAuth 2.0 Scope permissions, you should have them now. - Step 9. Go to the Team Members Tab, and make sure you appear as Team member. If you do not, add yourself as a team member.","title":"Step 5: Configure LinkedIn API"},{"location":"index.html#step-6-set-up-rapid-api-optional-if-want-to-use-rapid-api-sources","text":"Create an account on RapidAPI: - Step 1. Go to https://rapidapi.com/ and search for the following APIs: - Google Search API - Step 2. Subscribe to the Medium API: Go to https://rapidapi.com/nishujain199719-vgIfuFHZxVZ/api/medium2 and click on subscribe. - Step 3. Subscribe to the Google API: Go to https://rapidapi.com/rphrp1985/api/google-api31/ and click on subscribe. - Step 4: Subscribe to Youtube Transcribe API: Go to https://rapidapi.com/ and click subscribe - Step 5. Copy the key that it shows after subscribing to the API on the API Key Header, and set it in you vault as RAPID_API_KEY","title":"Step 6: Set Up Rapid API (Optional, if want to use Rapid API sources)"},{"location":"index.html#step-8-obtain-youtube-api-key-from-google-cloud-platform-optional-if-want-to-use-youtube-as-source","text":"","title":"Step 8: Obtain Youtube API Key from Google Cloud Platform (Optional, if want to use Youtube as source)"},{"location":"index.html#step-1-set-up-a-google-cloud-project","text":"Log in to Google Cloud Console Go to Google Cloud Console. Create a New Project Click on the \"Select a project\" dropdown in the top navigation bar. Click New Project. Enter a Project Name, choose your Billing Account (if prompted), and click Create.","title":"Step 1: Set Up a Google Cloud Project"},{"location":"index.html#step-2-enable-youtube-data-api-v3","text":"Navigate to the APIs & Services Dashboard In the left-hand menu, click APIs & Services > Library. Search for YouTube Data API v3 In the search bar, type YouTube Data API v3. Click on the API from the search results. Enable the API Click the Enable button to activate the YouTube Data API for your project.","title":"Step 2: Enable YouTube Data API v3"},{"location":"index.html#step-3-create-api-credentials","text":"Go to the Credentials Page In the left-hand menu, click APIs & Services > Credentials. Click \"Create Credentials\" In the top toolbar, click the \"Create Credentials\" button. Select API Key from the dropdown. Copy the API Key After creation, a pop-up will display your new API key. Click the Copy icon and save the key somewhere secure.","title":"Step 3: Create API Credentials"},{"location":"index.html#step-4-restrict-your-api-key-optional","text":"To ensure security, restrict how and where your API key can be used: Edit API Key Restrictions On the Credentials page, click the Edit icon next to your API key. Set Application Restrictions Under Application restrictions, select HTTP referrers (web sites). Add the URL of your website or application. Set API Restrictions Under API restrictions, select Restrict key. Select the YouTube Data API v3 from the dropdown. Click Save.","title":"Step 4: Restrict Your API Key (Optional)"},{"location":"index.html#step-9-set-up-desired-llm-providers","text":"Available supported providers are: OpenAI Groq Google Gen AI Deepseek You can obtain the API keys for these providers and set them in your Vault as OPENAI_API_KEY , GROQ_API_KEY , GOOGLE_GEN_AI_API_KEY , and DEEPSEEK_API_KEY respectively. Note: OpenAI is the only image model provider available at the moment. Therefore, it is mandatory to set the OPENAI_API_KEY in your Vault, if you want to use the image generation tool .","title":"Step 9: Set up desired LLM providers"},{"location":"index.html#step-10-install-c-build-tools-from-visual-studio-only-if-installing-as-windows-service","text":"Torch requires C++ Build Tools from Visual Studio to be installed. For this you can: 1. Install Visual Studio (I installed 2022) 2. Install the C++ Desktop Development Workload 3. Open a command prompt and run: where cl.exe to find the path to the C++ compiler 4. Add the path to the C++ compiler to your PATH environment variable 5. Run C:\\Program Files\\Microsoft Visual Studio xx.x\\VC\\vcvarsall.bat (where xx.x is the version of Visual Studio you installed) to set up the environment variables for the C++ compiler For my architecture and Windows SDK version (gets installed with the C++ Build Tools) I ran \\\"Program Files\"\\\"Microsoft Visual Studio\"\\2022\\Enterprise\\VC\\Auxiliary\\Build\\vcvarsall.bat X64 10.0.22621.0","title":"Step 10: Install C++ Build Tools from Visual Studio (Only if installing as Windows Service)"},{"location":"index.html#step-11-install-the-service","text":"If using Windows: Open root project folder on CMD Terminal with Admin Privileges and run Run .\\install.bat (If you want to rebuild the EXE file run with argument --rebuild) The program will ask for your user password in order to install the service with your account (It will not validate it but installation will fail at the end if incorrectly provided) If using Unix, on terminal, run: sudo ./install.sh","title":"Step 11: Install the service"},{"location":"index.html#author","text":"Daniel Cabrera Rodr\u00edguez GitHub : @dan415 Email : danicr2515@gmail.com Feel free to reach out with questions or suggestions!","title":"Author"},{"location":"index.html#license","text":"MIT License \u00a9 2024 Daniel Cabrera Rodr\u00edguez Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"core/llm/rationale.html","text":"LLM Components This section contains the documentation for the LLM module. The LLM module is composed of two components: The langchain agent , which is the main component, in charge of actually using ChatGPT in order to generate the content. The retrieval agent , which is in charge of converting the raw and extensive content material from the pdfs into a relevant context the agent can use in order to create a post. (For shorter sources this step is not used and the content is fed whole) Right at the moment, we consider the following LLM options: OpenAI (Chat models, base (instruct and Dall-E) models, embedding models) Google Gen AI (Chat models, base (instruct) models, embedding models) Groq (Chat Models) Obviously, for models to work you need to have defined their respective API keys inside the vault secrets. Respectively: GROQ_API_KEY , OPENAI_API_KEY , GOOGLE_API_KEY and have installed the dependencies for each one of them, respectively: langchain_groq , langchain_openai , langchain-google-genai NOTE: For image generation, only Dall-E has been tested. Compatibility for other image generation models from other providers will be ensured in following updates The design for constructing different LLM configurations is the following: NEW_OPTION = \"ALIAS-TYPE-PROVIDER\" or NEW_OPTION = \"ALIAS-PROVIDER\" for base models, meant for instruct operations or image generation models like Dall-E 3 Where ALIAS gives \"uniqueness\" to the configuration, and must be followed by a supported provider. As of now, openai , groq and google (Google Gen AI) are supported. Note: langchain only implements chat models for groq as of now. Then, we can write into the configuration any parameter that the model accepts, for example: { \"config_name\":\"mini-chat-openai\", \"temperature\":{\"$numberDouble\":\"0.5\"}, \"model_name\":\"gpt-4o-mini\", \"top_p\":{\"$numberInt\":\"1\"}, \"frequency_penalty\":{\"$numberDouble\":\"0.0\"}, \"presence_penalty\":{\"$numberDouble\":\"0.0\"} } or { \"config_name\":\"llama3370b-chat-groq\", \"temperature\":{\"$numberDouble\":\"0.5\"}, \"model_name\":\"llama-3.3-70b-versatile\", \"top_p\":{\"$numberInt\":\"1\"} } or { \"config_name\":\"gemini1.5-embeddings-google\", \"model\":\"models/embedding-001\" }","title":"Rationale"},{"location":"core/llm/conversation/rationale.html","text":"Langchain Agent The Langchain agent is based on a langchain react agent for LLM models. Therefore, the agent has access to tools. About the conversation history Each conversation thread is set to match the publication_id of one of the posts. For this, we use the method produce_publication which receives a publication with the publication id set. This method also adds int the system prompt and the publication. This system prompt is preserved throughout the conversation, as the trimming method is configured to maintain the system prompt. If you are familiar with the MemorySaver and BaseCheckpointSaver classes (Checkpointer classes) in LangGraph, what we do is carry out an implementation that achieves the checkpointing but instead of executing in memory, it does so in MongoDB. Therefore, all checkpoints are saved seamlessly inside MongoDB inside checkpoint_writes and checkpoints collections. This way, we can always surf through the different conversations simply defining the parameter to invoke : {\"configurable\": {\"thread_id\": \"whatever_publication_id\"}} Note that this unrelated to what langchain defines as \"long-term memory\". This is basic conversation history handling, but with the advance of always preserving the state as long as the publication is not published to LinkedIn or discarded. Upon the publication achieving one of these states, the conversation history is erased from database for storage optimization Response format LinkedIn does not allow for Markdown format, but I find that most llms are better at formatting their responses in Markdown. So what I do to still be able to write formatted posts is to use a formatter to bold Markdown bold strings as unicode bold characters. Multimodal input If the model admits multimodal input (text and images domains), you will be able to pass in an image apart from the text in the messages to the agent. Langchain Agent Configuration The configuration file must be contained inside the config collection with the field config_name : \"llm-conversation-agent\" . It contains the following parameters: max_conversation_length : The maximum length of the conversation. This is used to limit the memory of the agent at n previous messages. system_prompt_template : The system prompt template is used in conjunction with the actual post (in json format) in order to generate the content. It gives the model the required instructions tools : Can be a list combination of any prebuilt langchain agent + generate_image . This tool is the implemented form of generating an image and setting it as the post image. The react agent expects the find at least one tool to use, so this list cannot be empty. model_provider : The chat model provider, can be any supported chat model image_model_provider : Image model provider, as of now, only dalle-openai is supported image_generation_prompt : Prompt for generating the images trimming_strategy : Can be one of token message . apply_unicode_bold : If set to True, message formatting for changing bold MarkDown strings to unicode bold characters will be used max_tokens : Max tokens to pass to the model. It is used to trim the conversation history when trimming_strategy is set to token max_conversation_length : Number of max allowed messages to be passed to the model. It is used to trim the conversation history when trimming_strategy is set to message","title":"Conversations Agent"},{"location":"core/llm/model_providers/rationale.html","text":"Model Configurations Model configurations stored in the database are basically composed of two components: the config_name and the config_schema . The config_name is a string that identifies the configuration, while the rest of the document is the configuration itself and gets passed during the initialization of the model. Therefore, the configuration parameters are either keyword arguments for the model intialization or invocation. The config-name is used to identify the necessary python module and class. It can have 2 parts separated by a hyphen. The first part is the unique identifier part of the configuration. The second part is the provider of the configuration. For example, mini is the unique identifier and chat-openai is the provider. Available models are: chat-openai: OpenAI chat model chat-google: Google chat model chat-groq: Groq Chat model embeddings-openai: Embeddings OpenAI Gen AI class embeddings-google: Embeddings Google Gen AI class google: Base Google Gen AI class. Only suitable for completions API openai: Base OpenAI class. Only suitable for completions API dalle-openai: DALL-E OpenAI class (Langchain Wrapper) chat-custom: Uses Chat OpenAI as well Note: chat-custom is another alias for chat-openai . As OpenAI was the first chat model implemented, other providers follow the same standard, making it possible to use the same module for different providers. chat custom only requires two additional parameters are passed: - base_url : The URL of the chat model - api_key : The name of the API key secret in the vault Here is an example of a model configuration: { \"config_name\": \"mini-chat-openai\", \"temperature\": { \"$numberDouble\": \"0.5\" }, \"model_name\": \"gpt-4o-mini\", \"top_p\": { \"$numberInt\": \"1\" }, \"frequency_penalty\": { \"$numberDouble\": \"0.0\" }, \"presence_penalty\": { \"$numberDouble\": \"0.0\" } }","title":"Model Providers"},{"location":"core/llm/retrieval/rationale.html","text":"Langchain Agent The Langchain agent is based on a langchain Chain for OpenAI models (which will be extended for more models in the future). It is as well based on two components: - A chat prompt template consisting on a system prompt and a user prompt. - A memory, which is injected into the user prompt and stores the historical context of the conversation. Algorithm This algorithm is implemented as a langGraph workflow: START: Begin the workflow. Split Text: Input: Full text. Process: Split the input text into smaller, manageable chunks with overlap for context preservation. Output: List of text chunks. Create Documents: Input: List of text chunks. Process: Convert each chunk into a Document object with metadata (e.g., chunk order). Output: List of Document objects. Summarize Documents: Input: List of Document objects. Process: Use a language model to summarize the documents, combining partial summaries into a cohesive one. Output: Document summary (string). Generate Terms: Input: Document summary. Process: Generate a list of key terms or questions derived from the summary using a language model. Output: List of terms or questions. Create Vector Store: Input: List of Document objects. Process: Build a vector store using embeddings for efficient similarity searches. Output: Vector store (e.g., Chroma). Retrieve Relevant Chunks: Input: Vector store, list of terms or questions. Process: Perform similarity search for each term/question and retrieve relevant text chunks. Output: List of relevant text chunks. Answer Questions: Input: Relevant text chunks, list of terms or questions. Process: Use a language model to generate answers for each question based on the relevant chunks. Output: List of Q&A pairs (questions and their corresponding answers). Format Dialog: Input: List of Q&A pairs. Process: Format the questions and answers into a structured, readable dialog. Output: Formatted Q&A dialog (string). Extract Title: Input: Full text or summary. Process: Extract the document's title using a language model. Output: Extracted title (string). END: The workflow completes. Final Outputs Formatted Q&A Dialog: A structured conversation summarizing the document. Extracted Title: A concise title representing the document\u2019s content. Langchain Agent Configuration The configuration file must be contained inside the config collection with the field config_name : \"llm-retrieval-langchain\" . It contains the following parameters: chunk_size : chunk size for text splits used in the RecursiveCharacterTextSplitter chunk_overlap : chunk overlapping in between text splits used in the RecursiveCharacterTextSplitter embeddings_provider : embedding model configuration defined in configs model_provider : instruct or base model configuration defined in configs query_expansion_prompt : this prompt utilizes the text summary in order to produce n relevant questions about the text for building an article. The generated questions are then used for the semantic search algorithm answer_prompt : This prompt uses each generated question and the returned relevant chunks of text from the vector search in order to respond to the question title_extraction_prompt : This prompt takes the first chunk of text and tries to extract or infer the title for the source n_chunk_results : chunks to return for each semantic search for each of the questions Note: Most of the prompt templates are formatted strings and are to be completed dynamically, if wanting to edit the prompts, the text within {} must not be removed from the string.","title":"Retrieval Workflow"},{"location":"core/pdf/rationale.html","text":"PDF module This module is used to get the pdf text file. In the prior version I used Adobe PDF Extractor API. However, more often than not, I would run into API usage limits, so I decided it was not worth it to use it and switched to PyPDF. Recently, Docling library was released, bringing great capabilities with advanced extraction formats with OCR, from table structures, etc. So I wanted to include this as a brand-new option. Therefore, as of now, lightweight and straightforward Pypdf2 method and more complex and consuming Docling methods are supported. Arxiv and Manual PDFs search engines take the configuration field pdf_extractor_provider which can take one of pypdf or docling . Docling Configuration The Docling pdf extractor can be further customized in order to select specific functionalities of the library. The document is located inside the config collection the field config_name : \"docling\" . { \"config_schema\":\"docling\", \"do_table_structure\":true, \"do_ocr\":false, \"generate_picture_images\":true, \"table_former_mode\":\"accurate\" } do_table_structure : If set to True, table structures are extracted table_former_mode : can be either accurate or fast . If defines the method for extracting table structures do_ocr : If set to True, OCR recognition will be performed in order to extract text from images. Useful for scanned pdfs generate_picture_images : If set to True, images will be extracted and will be available to be selected for the post image All of these configurations affect content extraction performance and resource utilization.","title":"PDF Module"},{"location":"core/publications/rationale.html","text":"About publications One of the main features in V2.0 is the saving of publications in database. One of the main reasons for this is storing all the history. I figure it could be useful to derive tendencies and some form of recommendation system for the type of posts to promote to the user. Therefore, publications can have different state in database: DRAFT: Publication source content has been extracted but actual publication is yet to be produced PENDING_APPROVAL: Publication content has been produced and is available to review by the user PUBLISHED: Publication has been published to LinkedIn DISCARDED: Publication has been discarded by the user Other important metadata fields are: - last_updated : Last time the publication was updated in some way - creation_date : Creation datetime - image : Can contain the image bytes encoded in base 64 for the selected publication image All publications have the field publication_id . This ID not only uniquely identifies the actual post, but also identifies the conversation thread ID used by LangChain's memory saver object in order to keep track of the different conversations. Therefore, we can have as many different conversations as publications. Normally, LangChain conversations are kept in-memory, and lost after restart of the system, as they are not meant to \"out-live\" a session (except for the long-term memory, which is not related to this). For this use case, we do want to be able to recover the conversation and switch in between them as long as the publication has not been either published or discarded. So, our agent will always have the last messages available as context for the publication that we are working with at the moment. For managing posts, we use the PublicationsIterator class in order to perform CRUD operations on them, and to be able to access them in a circular linked list manner.","title":"Publications"},{"location":"information/rationale.html","text":"Information Searching Component This module is responsible for searching for information using different sources. Sources The following sources are currently supported: Arxiv Google News Medium Youtube urls Inputted PDFS (from a directory) The sources are handled by Sources Handler class. I intent to extend the sources to many more, and it is very easy to add more sources, thanks to the modular design of the program. Key aspects - All sources store their configurations in database. They get initialized before starting the search algorithm, and after the execution is done, the current configuration is updated with the values for the keys stored within the source object. - For a content to be considered as publication, the content must exceed the minimum_length . - The search algorithm is run every period days, relative to period_datetime . Configuration The configuration file must be contained inside the config collection with the field config_name : \"information\" . The following parameters are available: active_sources : A list of strings containing the names of the sources to be loaded. The names must match the Enum string values of the InformationSource class. execution_period : Time on days between executions. This is used by the Source Handler class. sleep_time : Number of execution periods to wait between searches. Only used by Source Handler class. process_sleep_time : The time in seconds to wait between processes. Only used by Publications Handler class. last_run_time : The last time the program was run. This is used to calculate the time elapsed since the last run, and to determine if the program should run again. one_by_one : If set to true, the program will run the sources one by one, instead of running them concurrently. This is useful for debugging purposes. active : If set to false, the program will not run. This is useful for debugging purposes. publications_collection : Name of the collection in Mongo DB that stores the publications","title":"About"},{"location":"information/producer/rationale.html","text":"Publications Handler This class is responsible for handling the publications. It is responsible for the following: Walk the publications_directory Process the publication, generating a publication idea using the LLM Module. Save the publication inside publications collection with state DRAFT It uses the information config document in the config collection in MongoDB","title":"Publications Producer"},{"location":"information/searcher/rationale.html","text":"Sources Handler The Sources Handler class is responsible for handling the sources. It is responsible for the following: Run search engines for each source asynchronously and concurrently. Sleep for a specified amount of time between searches. Supported information sources are loaded from config document in MongoDB with the field config_name : \"information\" . Sources defined in the config file need to exist as Enum Items of InformationSource class inside information/sources/information_source.py. The Enum string value is used in order to load the sources from: { \"active_sources\": [ \"medium\", \"arxiv\", \"google_news\", \"manual, youtube\" ] } Changing this list will change the sources that are loaded. The output from sources may vary from source to source, but retrieved information is always stored as json inside the publications_directory. Once they get processed by the Publications handler, they get moved to the pending approval directory.","title":"Content Searcher"},{"location":"information/sources/arxiv/rationale.html","text":"Arxiv This Content Search Engine is based on the Arxiv API . The API is used to retrieve the information of the papers, and the search engine is used to index the information and provide a search interface. It searches papers in the Arxiv API, filtering by by the topics specified in config, including results only from last week and with a set maximum of results. The Arxiv API returns the papers' metadata in XML format, which is then converted to json. From there, we also download the PDF bytes of the paper and pass it directly to the Adobe PDF Services API. After that, we add the paper content to the json, and index using the ColBert model. Then the model is queried with the colbert queries in order to retrieve the meaningful information that we want to use for generating the publication later on. Configuration The configuration file is located inside the config collection the field config_name : \"information-sources-arxiv\" . It contains the following parametes: max_results : The maximum number of results to retrieve from the API. url : The URL of the Arxiv API. minimum_length : The minimum length of the contents. Papers with content less than this number of characters are filtered out paragraph_min_length : The minimum length of the paragraphs. Paragraphs with content less than this number of characters are filtered out topics : A list of topics to search for. The topics are used to filter the results from the API. Only papers that contain at least one of the topics are retrieved. Some of these topic codes are: cat:cs.AI: Artificial Intelligence cat:cs.CL: Computation and Language cat:cs.GT: Computer Science and Game Theory\u00e7 cat:cs.CV: Computer Vision and Pattern Recognition cat:cs.ET: Emerging Technologies cat:cs.IR: Information Retrieval cat:cs.LG: Machine Learning cat:cs.NE: Neural and Evolutionary Computing cat:cs.PL: Programming Languages cat:cs.RO: Robotics period : The period of time to search for in days. provider : Right now it can only be Langchain-RAG . This defines the RAG agent used to extract the context that will be used in order to generate publications. ColBert has been deprecated and as of now only Langchain-RAG is supported pdf_extractor_provider : Can be either docling or pypdf . More detailed information on the different pdf extractor providers on the pdf section. Algorithm Build Search Query : Construct the query URL using: Topics (categories to search for). Time period (date range of interest). Sorting preference (e.g., relevance). Maximum number of results to fetch. Send API Request : Perform an HTTP GET request to the Arxiv API using the constructed query URL. If the request fails or returns a non-200 status code, log an error and stop processing. Extract Metadata : Parse the XML response using extract_from_xmls . Extract metadata for each paper, including title, authors, publication date, summary, and link. Process Each Paper : For each extracted paper: Convert the paper's abstract link to a PDF download link. Download the PDF file. Extract text content from the PDF using the PDFExtractorProvider . Use the DocumentRetrieverProvider to process the text, identifying important paragraphs. Append the processed content to the paper's metadata. Save Valid Results : If a save_callback is provided, validate and save each processed paper. Return Results : Compile all successfully processed papers into a single list. Return the list of results.","title":"Arxiv"},{"location":"information/sources/manual_pdfs/rationale.html","text":"Manual PDF Input This Content Search Engine is used to create posts from papers that you want to force the agent to process. The way to do this would be to add the PDF inside the designated input folder. Then, when the Sources Handler runs, it will detect the PDF and will create a post from it. The post will be stored in the pending approval folder. The agent reads from the input folder, and pass the papers directly to the Adobe PDF Services API. After that, we add the paper content to the json, and index using the ColBert model. Then the model is queried with the colbert queries in order to retrieve the meaninful information that we want to use for generating the publication later on. The sources handler will save the post in the pending approval folder, and will move the actual pdf from the input directory to the output directory. This is just to keep the input directory clean, and to avoid processing the same pdfs over and over again, and not to delete the pdfs. Also, it serves as a method to keep track of the pdfs that have been processed. Configuration The configuration file is located inside the config collection the field config_name : \"information-sources-manual_pdf\" . It contains the following parametes: minimum_length : The minimum length of the contents. Papers with content less than this number of characters are filtered out paragraph_min_length : The minimum length of the paragraphs. Paragraphs with content less than this number of characters are filtered out input_directory : The directory in BlackBlaze B2 where the PDFs to be processed are located. Defaults to Information/Sources/Manual/Input output_directory : The directory in BlackBlaze B2 where the PDFs that have been processed are moved to. Defaults to Information/Sources/Manual/Output provider : Right now it can only be Langchain-RAG . This defines the RAG agent used to extract the context that will be used in order to generate publications. ColBert has been deprecated and as of now only Langchain-RAG is supported pdf_extractor_provider : Can be either docling or pypdf . More detailed information on the different pdf extractor providers on the pdf section. Algorithm Initialization : Authenticate with the B2 storage service using the pdf_manager . Retrieve a list of files from the input_directory in BlackBlaze B2 Filter the files to identify PDFs. Iterate Over PDFs : For each PDF in the filtered list: Download Content : Use the get_pdf_content method to download the PDF content as raw bytes. Extract Information : Use the extract_pdf_info method to extract metadata and relevant content from the PDF. Save Valid Content : If a save_callback is provided, validate and save the extracted information. Move Processed PDF : Move the processed PDF file from the input_directory to the output_directory . (Located in BlackBlaze B2) Return Results : Combine all successfully processed and extracted content into a single list. Return the list of extracted results.","title":"Manual PDF Input"},{"location":"information/sources/rapid/rationale.html","text":"Rapid Sources This Content Search Engine is based on the Rapid API . This is the base class for each of the rapid sources. All the Rapid API sources share the same API key. For this reason, all Rapid API requests are wrapped with a decorator that retrieves the API Key and counts the number of API calls. The n\u00ba of API calls on the other hand is not shared by the different Rapid API services, so the count_requests field is stored for each one of the rapid API sources. Therefore, even though all rapid API sources share the same API key, they do not share the same usage limit and calls to one API servivce do not affect the limit for other services.","title":"About"},{"location":"information/sources/rapid/medium/rationale.html","text":"Medium This Content Search Engine is based on the Rapid API . This Content Search Engine is used to retrieve information from Medium articles. Configuration The configuration file is located inside the config collection the field config_name : \"information-sources-rapid-medium\" . It contains the following parameters: limit : The maximum number of results to retrieve from the API. period : The period of time to search for in days. url : The URL of the Rapid API source. host : The host of the Rapid API source. max_results : The maximum number of results to retrieve from the API. minimum_length : The minimum length of the contents. Papers with content less than this number of characters are filtered out count_requests : An integer counter that keeps track of the number of requests made to the API. This is used to keep track of the number of requests made to the API, and to disable the source when the limit is reached. topics : A list of topics to search for. Every topic is a \"normal\" passed to medium's search engine. Algorithm Initialization : Start with a list of topics to search. Prepare an empty list to store all results. Iterate Over Topics : For each topic: Call the research_topic method to fetch related Medium articles. Research a Topic : Query Medium's API with the topic to retrieve a list of articles. For each article: Retrieve article metadata using get_article_info (e.g., title, subtitle, author, publication date). Fetch the article's content using get_article_content . Enrich the article with additional metadata (e.g., information source). Append the processed article to the results list. If a save_callback is provided, validate and save the processed article. Return Results : Combine all results from processed topics into a single list. Return the list of results.","title":"Medium"},{"location":"information/sources/rapid/news/rationale.html","text":"Google News This Content Search Engine is based on the Rapid API . This Content Search Engine is used to retrieve information from Google News. Configuration The configuration is located inside the config collection the field config_name : \"information-sources-rapid-google_news\" . It contains the following parameters: limit : The maximum number of results to retrieve from the API. period : The period of time to search for in days. url : The URL of the Rapid API source. host : The host of the Rapid API source. max_results : The maximum number of results to retrieve from the API. minimum_length : The minimum length of the contents. Papers with content less than this number of characters are filtered out count_requests : An integer counter that keeps track of the number of requests made to the API. This is used to keep track of the number of requests made to the API, and to disable the source when the limit is reached. topics : A list of topics to search for. Every topic is a \"normal\" passed to medium's search engine. Algorithm Initialization : Start with a list of topics to search. Prepare an empty list to store all results. Iterate Over Topics : For each topic: Create a payload with search parameters (e.g., topic text, region, and maximum results). Make an API request to fetch Google News results. Process Each Result : For each result in the API response: Extract and format key fields (e.g., summary, link). Use the get_text method to fetch and extract the full article content from the result's URL. Enrich the result with additional metadata (e.g., information source). Save Processed Results : If a save_callback function is provided: Validate the processed result. Save the result using the callback. Return Results : Combine all successfully processed results into a single list. Return the list of results.","title":"Rationale"},{"location":"information/sources/rapid/youtube/rationale.html","text":"Google News This Content Search Engine is based on the Rapid API . This Content Search Engine is used to retrieve information from Google News. Configuration The configuration file is located inside the config collection the field config_name : \"information-sources-rapid-youtube\" . It contains the following parameters: limit : The maximum number of results to retrieve from the API. period : The period of time to search for in days. url : The URL of the Rapid API source. host : The host of the Rapid API source. minimum_length : The minimum length of the contents. Papers with content less than this number of characters are filtered out count_requests : An integer counter that keeps track of the number of requests made to the API. This is used to keep track of the number of requests made to the API, and to disable the source when the limit is reached. Algorithm The URL pool is made of urls sent by the user through the bot. Whenever the searching algorithm starts, it executes: Start with the URL pool. For each URL: Retrieve video metadata. Fetch the transcript using an API. Download a thumbnail (if possible). Organize this data into a structured format. Save the result (if required). Pop URL from pool Continue until all URLs in the pool are processed.","title":"YouTube"},{"location":"linkedin/rationale.html","text":"LinkedIn This module is used to integrate with LinkedIn. It is used to authenticate the user and to give access to the LinkedIn API to post content. Authentication This module exposes two REST methods to authenticate the user with LinkedIn. The first one is used to get the URL to redirect the user to, and the second one is used to get the access token after the user has been redirected back to the application. The authentication process is as follows: 1. The user tries to publish content to LinkedIn. 2. The application checks if the user is authenticated. If not, it redirects the user to the authentication URL, exposed on \"/\". This sends the auth request to LinkedIn using the client_id and secret of the application. 3. After the user is redirected to the LinkedIn Authentication page, the user needs to log in with an account that is member of the page. 4. After the user logs in, LinkedIn redirects the user back to the application, with the access token as a parameter at \"/callback\". 5. The Callback saves the access token in the LinkedIn config file, and then the user can finally prompt to publish again, this time succesfully. The LinkedIn publisher class manages the LinkedIn API in order to publish posts. Posts can now contain an image. For this, we first need to register the image as an asset, upload the asset, and use the asset resource identification to include it in the post as an image. Configuration The configuration file must be contained inside the config collection with the field config_name : \"linkedin\" . It contains the following parameters: footer : The footer to add to the post. This is used to add a footer to the post. I use it to specify that the post was posted automatically by an AI","title":"LinkedIn"},{"location":"main/rationale.html","text":"Main Modules This module only contains the main script that is run in order to run the different services that form the application. It creates the services as asyncio tasks and runs them in parallel. Tasks The main tasks that are run by this module are the following: Auth server: This task runs the authentication flask server that is used to authenticate users and to generate the access tokens that are used to publish on LinkedIn. Bot Agent: The Telegram Bot Publications Handler: This task is responsible for publishing the publications on LinkedIn Sources Handler: This task is responsible for retrieving the publications from the different sources","title":"Main Component"},{"location":"release_notes/rationale.html","text":"Release Notes v2.0.0 Welcome to v2.0.0 \u2014a transformative update that redefines the capabilities of the LinkedIn Gen AI Posting Creator Assistant. This major release introduces groundbreaking features, elevates functionality, and delivers seamless integration with modern cloud services. Here\u2019s what\u2019s new: What's changed External Configuration Management : All state configurations are now hosted externally, significantly boosting maintainability and portability. MongoDB Atlas : Manages configurations, conversation histories, and publications (free up to 25GB). Backblaze B2 : Cloud storage for PDF files and images (free up to 10GB). HashiCorp Vault : Secure storage for secrets and credentials used (free for static secrets). Hashicorp vault credentials are now stored in the system's keyring. New available source : From YouTube urls: Simply pass YouTube URLs to the bot. The bot will extract transcripts, metadata, and thumbnails to craft compelling LinkedIn posts. Improved PDF text extraction : Docling : Enhanced PDF text extraction for more accurate and insightful post generation. PyMuPDF : Added as lightweight PDF extraction alternative. Extended LLM support : Google Gen AI Studio Integration : Access to Google Gen AI Studio models, including latest Flash 2 model. Groq : Integration with all Groq models Deepseek : Integration with Deepseek models, including the latest Deepseek V3 model. Custom models : Easily add custom models to the bot if compatible with the OpenAI API Enhanced Post Generation : AI-Generated Art : Create custom visuals using DALL-E 3\u2019s powerful AI image generation. Image Integration for Posts : Use YouTube video thumbnails, embedded source images, or manually uploaded images as post visuals. Improved Retrieval Pipeline : LangChain RAG workflow : Replaces the legacy ColBERT-based retrieval system, offering advanced vector search capabilities. New Bot Commands Manual Uploads : Send PDF documents directly to the bot to add them to the queue Image Management : Upload, list, and remove images for posts YouTube Integration : Add YouTube URLs to the bot for post creation Search Engine Activation : Enable or disable the search engine for post generation Agent Boosted Capabilities Tool Usage : Access to all pre-built LangChain tools for enhanced post creation, like accessing the internet, generate images or acccess arxiv papers. Easier installation : Only need to set up external services and vault secrets prior to installation. Added Docker Support : Run the system with Docker for easy deployment and management. Insallation scripts : Added installation scripts for easy setup and configuration for both Linux and Windows Improved Logging : Enhanced logging for better debugging and monitoring.","title":"Release Notes"},{"location":"release_notes/rationale.html#release-notes","text":"","title":"Release Notes"},{"location":"release_notes/rationale.html#v200","text":"Welcome to v2.0.0 \u2014a transformative update that redefines the capabilities of the LinkedIn Gen AI Posting Creator Assistant. This major release introduces groundbreaking features, elevates functionality, and delivers seamless integration with modern cloud services. Here\u2019s what\u2019s new:","title":"v2.0.0"},{"location":"release_notes/rationale.html#whats-changed","text":"External Configuration Management : All state configurations are now hosted externally, significantly boosting maintainability and portability. MongoDB Atlas : Manages configurations, conversation histories, and publications (free up to 25GB). Backblaze B2 : Cloud storage for PDF files and images (free up to 10GB). HashiCorp Vault : Secure storage for secrets and credentials used (free for static secrets). Hashicorp vault credentials are now stored in the system's keyring. New available source : From YouTube urls: Simply pass YouTube URLs to the bot. The bot will extract transcripts, metadata, and thumbnails to craft compelling LinkedIn posts. Improved PDF text extraction : Docling : Enhanced PDF text extraction for more accurate and insightful post generation. PyMuPDF : Added as lightweight PDF extraction alternative. Extended LLM support : Google Gen AI Studio Integration : Access to Google Gen AI Studio models, including latest Flash 2 model. Groq : Integration with all Groq models Deepseek : Integration with Deepseek models, including the latest Deepseek V3 model. Custom models : Easily add custom models to the bot if compatible with the OpenAI API Enhanced Post Generation : AI-Generated Art : Create custom visuals using DALL-E 3\u2019s powerful AI image generation. Image Integration for Posts : Use YouTube video thumbnails, embedded source images, or manually uploaded images as post visuals. Improved Retrieval Pipeline : LangChain RAG workflow : Replaces the legacy ColBERT-based retrieval system, offering advanced vector search capabilities. New Bot Commands Manual Uploads : Send PDF documents directly to the bot to add them to the queue Image Management : Upload, list, and remove images for posts YouTube Integration : Add YouTube URLs to the bot for post creation Search Engine Activation : Enable or disable the search engine for post generation Agent Boosted Capabilities Tool Usage : Access to all pre-built LangChain tools for enhanced post creation, like accessing the internet, generate images or acccess arxiv papers. Easier installation : Only need to set up external services and vault secrets prior to installation. Added Docker Support : Run the system with Docker for easy deployment and management. Insallation scripts : Added installation scripts for easy setup and configuration for both Linux and Windows Improved Logging : Enhanced logging for better debugging and monitoring.","title":"What's changed"},{"location":"telegram/rationale.html","text":"Telegram Bot The idea of this bot is that it will allow to interact with the system from a Telegram client. The bot will allow to chat with the LLM conversation agent, receive publication suggestions, ask the agent to change the publication drafts, discard drafts, and publish them. The bot is based on the Origami Telegram Bot package, used for building Telegram bots in Python. In order to be able to circle through conversation threads, the memory pickle of the LLM agents is actually the object that is stored on the pending for approval directory. These are loaded and maintained in a circular list of suggestions. The bot will then allow to move forward and backwards through the list of suggestions, and to approve, discard them or change them. Publications Pool The bot has access to all publications in state PENDING , which means that they are eligible for review, posting or otherwise discarded. The posts are accessed with an iterator that retrieves the posts ordered by creation_date . The pool is circular, meaning that if commands /last or /next are called on an edge post in the list, the index is set to the contrary edge post in the list. Algorithm The algorithm is as follows: While True: 1. Check if the bot has just published. If so, wait for suggestion_period days. 2. Check if suggestions are blocked. If so, wait for 5 minutes in order to check again 3. If suggestions are not blocked, update the suggestions and check if there are suggestions. 4. If there are suggestions, send the current suggestion and block suggestions. Then the flow of the program is carried by the interaction with the user via Telegram. About the messages The bot is able to process text messages along with images. If you send a pdf file to the bot, the bot will assume you want the pdf to be saved for the manual PDF extraction algorithm, and will be saved in BlackBlaze B2. Bot State This class is used to store the state of the bot. It is used to store the chat id, the current suggestion, and the suggestions are blocked or not. For every publication (suggestion) there needs to be a conversation thread. Via Bot commands I can change the conversation thread, so I can finish \"tuning\" the publication and then publish it. As said on the Algorithm section, suggestions can be blocked. This is done in order to avoid the bot to send suggestions when the user is already interacting with the bot. The bot will send a suggestion, and then block suggestions until the user has finished interacting with the bot. This is done by setting the suggestions_blocked field to True. When the user finishes interacting with the bot, the suggestions_blocked field is set to False, and the bot will send a new suggestion if there is one available. Also, if the sending of suggestions fails, suggestions get blocked in order to avoid the bot sending failing continuously. The Bot is now designed to respond to one person only. It obviously could respond to whoever sent messages to it, but in order to send suggestions (which do not require interaction with the user) it needs to know the chat id of the user. This is why the bot is designed to respond to one person only. This is the purpose of the TELEGRAM_CHAT_ID secret. It will be first set on /start command and it is to remain constant afterwards. Setting the chat id as a secret makes the solution more robust to unauthorized access to the bot. The suggestions are in order, so I can go to the next or previous suggestion. I can also select a suggestion by index. Also, this state also saves information about when to automatically make suggestions, which would be when no suggestion is selected and after some time after a publication is made. The stateful decorator is used to update the config file after a function is called, so the file is always updated with the object state. About restricted access Access is restricted to only one chat ID ( TELEGRAM_CHAT_ID ), set on /start command and stored in the vault for secrets. After that, only messages received from that chat will be attended This is handled with the @restricted decorator Bot Commands /start : Starts the bot. It will set the allowed chat_id if not already set , send a welcome message and a suggestion if there is one available. /next : Sends the next suggestion (and changes the thread of conversation) if there is one available. /previous : Sends the previous suggestion (and changes the thread of conversation) if there is one available. /select {index} : Selects a suggestion by index. It will send the selected suggestion and change the thread of conversation. /list : Lists the suggestions in the pool with their index. /publish : Publishes the current suggestion to LinkedIn. If not authenticated, it will send a message with the link to the authentication page. /allow : Allows the bot to send suggestions. /stop : Blocks the bot from sending suggestions. /healthcheck : Sends a message to the user (to see if the bot is alive). /clear : Discards the current suggestion. /clear_image : It will remove the current image for the post /set_image : Used along with an image in the message, it will set the passed image for the post to be published /add_youtube {url} : Adds a youtube url to the youtube url pool in order for the video to get processed into a new post /update : Updates the current publication's content with the last message sent by the bot. /current : Sends the current suggestion. This is what will get published if the user uses the /publish command. /activate_search_engine : ables the search engine to execute /stop_search_engine : disables the search engine to execute /images : Lists the images available for the current post Bot Configuration The configuration file must be contained inside the config collection with the field config_name : \"telegram\" . It contains the following parameters: suggestion_period : N\u00ba of days to wait after a post has been published in order to start suggesting new posts again cool_off_time : Time to wait in order to send a new publication to the user after the last one was published","title":"Telegram Bot"},{"location":"todo/rationale.html","text":"Next steps I would like to implement the following features: Expand to other Social Media platforms. (Twitter, Instagram...) Next release I will be adding docker support. GUI for easy configuration so that users do not have to manually set up environment variables and can change most used settings on the fly. Meta filterer/selector algorithm to filter and order the posts based on the user's preferences. Might implement it as an agent or RL Implement calling telegram bot commands as natural language through the agent Implement GitHub repositories as a source These are ideas that I have in mind, but I am open to suggestions and contributions. However, I am busy, so I cannot guarantee that I will implement them in the near future. If you want to contribute, or have any ideas, please do not hesitate to contact me.","title":"Next Steps"},{"location":"windows/rationale.html","text":"Windows Service The LinkedinAssistantService class is designed to run the LinkedIn Assistant as a Windows Service. This service ensures that the LinkedIn Assistant runs continuously in the background and can be managed by the Windows Service Manager. It includes methods for initializing the service, handling accepted controls, and running the service in a threaded environment. The service can automatically restart on failure and perform cleanup operations when stopped. Basically, the service is a wrapper around the LinkedIn Assistant main module. This service is meant to be installed and executed as the user's account, and not with local system account, as it requires accessing Windows Credential Manager to store and retrieve the Hashicorp Vault credentials (which cannot be accessed by the local system account).","title":"Windows Service"}]}